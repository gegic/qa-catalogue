#!/usr/bin/env bash
set -uo pipefail

SECONDS=0
PREFIX=${BASE_INPUT_DIR}/_reports/${NAME}
DATE=$(date +"%F")
if [[ "${VERSION:-}" != "" ]]; then
  OUTPUT_DIR=${BASE_OUTPUT_DIR}/${NAME}-${VERSION}
else
  OUTPUT_DIR=${BASE_OUTPUT_DIR}/${NAME}
fi

if [[ "$1" != "help" && "$1" != "export-schema-files" ]]; then
  echo "OUTPUT_DIR: ${OUTPUT_DIR}"
  if [[ ! -d $PREFIX ]]; then
    mkdir -p $PREFIX
  fi

  # printf "%s %s> Logging to ${PREFIX}.log"

  if [ ! -d ${OUTPUT_DIR} ]; then
    mkdir -p ${OUTPUT_DIR}
  fi
fi

# die if input files don't exist
echo BASE_PICA_INPUT_DIR: ${BASE_PICA_INPUT_DIR}
echo ${MARC_DIR}/${MASK}
ls ${MARC_DIR}/${MASK} > /dev/null

do_validate() {
  GENERAL_PARAMS="--details --trimId --summary --format csv --defaultRecordType BOOKS"
  OUTPUT_PARAMS="--outputDir ${OUTPUT_DIR} --detailsFileName issue-details.csv --summaryFileName issue-summary.csv"

  printf "%s %s> [validate]\n" $(date +"%F %T")
  printf "%s %s> ./validate -Xms8g ${GENERAL_PARAMS} ${OUTPUT_PARAMS} ${TYPE_PARAMS} ${MARC_DIR}/${MASK} 2> ${PREFIX}/validate.log\n" $(date +"%F %T")
  ./validate ${GENERAL_PARAMS} ${OUTPUT_PARAMS} ${TYPE_PARAMS} ${MARC_DIR}/$MASK 2> ${PREFIX}/validate.log
}

do_prepare_solr() {
  printf "%s %s> [prepare-solr]\n" $(date +"%F %T")
  printf "%s %s> ./prepare-solr ${NAME} 2> ${PREFIX}/solr.log\n" $(date +"%F %T")
  ./prepare-solr $NAME 2> ${PREFIX}/solr.log
}

do_index() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--ignorableIssueTypes [^ ]+//g')
  printf "%s %s> [index]\n" $(date +"%F %T")
  printf "%s %s> ./index --db $NAME --file-path ${MARC_DIR} --file-mask $MASK ${PARAMS} --trimId 2> ${PREFIX}/solr.log\n" $(date +"%F %T")
  ./index --db $NAME --file-path ${MARC_DIR} --file-mask $MASK ${PARAMS} --trimId 2>> ${PREFIX}/solr.log
}

do_completeness() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  printf "%s %s> [completeness]\n" $(date +"%F %T")
  printf "%s %s> ./completeness --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/completeness.log\n" $(date +"%F %T")
  ./completeness --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/completeness.log
}

do_classifications() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  printf "%s %s> [classifications]\n" $(date +"%F %T")
  printf "%s %s> ./classifications --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/classifications.log\n" $(date +"%F %T")
  ./classifications --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/classifications.log
  printf "%s %s> Rscript scripts/classifications/classifications-type.R ${OUTPUT_DIR} 2>> ${PREFIX}/classifications.log\n" $(date +"%F %T")
  Rscript scripts/classifications/classifications-type.R ${OUTPUT_DIR}
}

do_authorities() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  printf "%s %s> [authorities]\n" $(date +"%F %T")
  printf "%s %s> ./authorities --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/authorities.log\n" $(date +"%F %T")
  ./authorities --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/authorities.log
}

do_tt_completeness() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  printf "%s %s> [tt-completeness]\n" $(date +"%F %T")
  printf "%s %s> ./tt-completeness --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ --trimId ${MARC_DIR}/${MASK} 2> ${PREFIX}/tt-completeness.log\n" $(date +"%F %T")
  ./tt-completeness --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ --trimId ${MARC_DIR}/${MASK} 2> ${PREFIX}/tt-completeness.log

  printf "%s %s> Rscript scripts/tt-histogram/tt-histogram.R ${OUTPUT_DIR} &>> ${PREFIX}/tt-completeness.log\n" $(date +"%F %T")
  Rscript scripts/tt-histogram/tt-histogram.R ${OUTPUT_DIR} &>> ${PREFIX}/tt-completeness.log

  # for large files
  # printf "%s %s> php scripts/tt-histogram/tt-histogram.php ${OUTPUT_DIR} &>> ${PREFIX}/tt-completeness.log\n" $(date +"%F %T")
  # php scripts/tt-histogram/tt-histogram.php ${OUTPUT_DIR} &>> ${PREFIX}/tt-completeness.log
}

do_shelf_ready_completeness() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  printf "%s %s> [shelf-ready-completeness]\n" $(date +"%F %T")
  printf "%s %s> ./shelf-ready-completeness --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ --trimId ${MARC_DIR}/${MASK} 2> ${PREFIX}/shelf-ready-completeness.log\n" $(date +"%F %T")
  ./shelf-ready-completeness \
    --defaultRecordType BOOKS \
    ${PARAMS} \
    --outputDir ${OUTPUT_DIR}/ \
    --trimId ${MARC_DIR}/${MASK} 2> ${PREFIX}/shelf-ready-completeness.log

  printf "%s %s> Rscript scripts/shelf-ready/shelf-ready-histogram.R ${OUTPUT_DIR} &>> ${PREFIX}/shelf-ready-completeness.log\n" $(date +"%F %T")
  Rscript scripts/shelf-ready/shelf-ready-histogram.R ${OUTPUT_DIR} &>> ${PREFIX}/shelf-ready-completeness.log

  # for large files
  # printf "%s %s> php scripts/shelf-ready-histogram.php ${OUTPUT_DIR} &>> ${PREFIX}/shelf-ready-completeness.log\n" $(date +"%F %T")
  # php scripts/shelf-ready-histogram.php ${OUTPUT_DIR} &>> ${PREFIX}/shelf-ready-completeness.log
}

do_bl_classification() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  printf "%s %s> [bl-classification]\n" $(date +"%F %T")
  printf "%s %s> ./bl-classification --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ --trimId ${MARC_DIR}/${MASK} 2> ${PREFIX}/bl-classification.log\n" $(date +"%F %T")
  ./bl-classification \
    --defaultRecordType BOOKS \
    ${PARAMS} \
    --outputDir ${OUTPUT_DIR}/ \
    --trimId ${MARC_DIR}/${MASK} 2> ${PREFIX}/bl-classification.log
}

do_serial_score() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  printf "%s %s> [serial-score]\n" $(date +"%F %T")
  printf "%s %s> ./serial-score --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ --trimId ${MARC_DIR}/${MASK} 2> ${PREFIX}/serial-score.log\n" $(date +"%F %T")
  ./serial-score --defaultRecordType BOOKS \
                 ${PARAMS} \
                 --outputDir ${OUTPUT_DIR}/ \
                 --trimId ${MARC_DIR}/${MASK} 2> ${PREFIX}/serial-score.log

  printf "%s %s> Rscript scripts/serial-score/serial-score-histogram.R ${OUTPUT_DIR} &>> ${PREFIX}/serial-score.log\n" $(date +"%F %T")
  Rscript scripts/serial-score/serial-score-histogram.R ${OUTPUT_DIR} &>> ${PREFIX}/serial-score.log
}

do_format() {
  ./formatter --defaultRecordType BOOKS ${MARC_DIR}/${MASK}
}

do_functional_analysis() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  printf "%s %s> [functional-analysis]\n" $(date +"%F %T")
  printf "%s %s> ./functional-analysis --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/functional-analysis.log\n" $(date +"%F %T")
  ./functional-analysis --defaultRecordType BOOKS \
                        ${PARAMS} \
                        --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/functional-analysis.log
}

do_network_analysis() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  printf "%s %s> [network-analysis]\n" $(date +"%F %T")
  printf "%s %s> ./network-analysis --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/network-analysis.log\n" $(date +"%F %T")
  ./network-analysis --defaultRecordType BOOKS \
                     ${PARAMS} \
                     --outputDir ${OUTPUT_DIR}/ \
                     ${MARC_DIR}/${MASK} 2> ${PREFIX}/network-analysis.log

  # network.csv (concept, id) ->
  #   network-by-concepts.csv (concept, count, ids)
  #   network-by-record.csv (id, count, concepts)
  #   network-statistics.csv (type, total, single, multi)
  printf "%s %s> Rscript scripts/network-transform.R ${OUTPUT_DIR} &>> ${PREFIX}/network-analysis.log\n" $(date +"%F %T")
  Rscript scripts/network-transform.R ${OUTPUT_DIR} &>> ${PREFIX}/network-analysis.log

  # network-by-concepts (concept, count, ids) ->
  #   network-pairs.csv (id1 id2)
  #   network-nodes.csv (id, id)
  printf "%s %s> ./network-analysis --outputDir ${OUTPUT_DIR} --action pairing --group-limit 2000 &>> ${PREFIX}/network-analysis.log\n" $(date +"%F %T")
  ./network-analysis --outputDir ${OUTPUT_DIR} \
                     --action pairing \
                     &>> ${PREFIX}/network-analysis.log

  cat network-pairs.csv | sort | uniq -c | sort -nr > network-pairs-uniq-with-count.csv
  awk '{print $2 " " $3}' network-pairs-uniq-with-count.csv > network-pairs-all.csv

  printf "%s %s> ziping output\n" $(date +"%F %T")
  PWD=`pdw`
  cd ${OUTPUT_DIR}
  zip network-input network-nodes.csv network-nodes-???.csv network-pairs-???.csv network-by-concepts-tags.csv
  cd $PWD


  printf "%s %s> upload output\n" $(date +"%F %T")
  scp ${OUTPUT_DIR}/network-input.zip pkiraly@roedel.etrap.eu:/roedel/pkiraly/network/input

  # spark-shell -I scripts/network.scala --conf spark.driver.metadata.qa.dir="${OUTPUT_DIR}"
  # ./network-export.sh ${OUTPUT_DIR}
}

do_pareto() {
  printf "%s %s> [pareto]\n" $(date +"%F %T")
  printf "%s %s> Rscript scripts/pareto/frequency-range.R ${OUTPUT_DIR} &> ${PREFIX}/pareto.log\n" $(date +"%F %T")
  Rscript scripts/pareto/frequency-range.R ${OUTPUT_DIR} &> ${PREFIX}/pareto.log

  . ./common-variables
  if [[ "${WEB_DIR:-}" != "" ]]; then
    if [[ ! -d $WEB_DIR/images ]]; then
      mkdir $WEB_DIR/images
    fi
    ln -s ${OUTPUT_DIR}/img $WEB_DIR/images/${NAME}
  fi
}

do_marc_history() {
  if [ "${SCHEMA:-}" == "PICA" ]; then
    SELECTOR='011@$a;001A$0|extractPicaDate'
  else
    SELECTOR="008~7-10;008~0-5"
  fi
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  printf "%s %s> [marc-history]\n" $(date +"%F %T")
  printf "%s %s> ./formatter --selector \"$SELECTOR\" --defaultRecordType BOOKS ${PARAMS} --separator \",\" --outputDir ${OUTPUT_DIR}/ --fileName "marc-history.csv" ${MARC_DIR}/${MASK} &> ${PREFIX}/marc-history.log\n" $(date +"%F %T")
  ./formatter --selector "$SELECTOR" --defaultRecordType BOOKS ${PARAMS} --separator "," \
              --outputDir ${OUTPUT_DIR}/ --fileName "marc-history.csv" ${MARC_DIR}/${MASK} &> ${PREFIX}/marc-history.log

    # | grep -v '008~7-10,008~0-5' \
  tail -n +2 ${OUTPUT_DIR}/marc-history.csv \
    | sort \
    | uniq -c \
    | sed -r 's/([0-9]) ([0-9uc xwticrka])/\1,\2/' \
    | sed 's, ,,g' > ${OUTPUT_DIR}/marc-history-groupped.csv 

  printf "%s %s> Rscript scripts/marc-history/marc-history-groupped.R ${OUTPUT_DIR} &>> ${PREFIX}/marc-history.log\n" $(date +"%F %T")
  Rscript scripts/marc-history/marc-history-groupped.R ${OUTPUT_DIR} &>> ${PREFIX}/marc-history.log
}

do_record_patterns() {
  printf "%s %s> [record-patterns]\n" $(date +"%F %T")
  printf "%s %s> Rscript scripts/top-fields.R ${OUTPUT_DIR} &>> ${PREFIX}/top-fields.log\n" $(date +"%F %T")
  Rscript scripts/record-patterns/top-fields.R ${OUTPUT_DIR} &>> ${PREFIX}/top-fields.log

  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  printf "%s %s> ./record-patterns --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} &> ${PREFIX}/record-patterns.log\n" $(date +"%F %T")
  ./record-patterns --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} &> ${PREFIX}/record-patterns.log

  head -1 ${OUTPUT_DIR}/record-patterns.csv | sed -e 's/^/count,/' > ${OUTPUT_DIR}/record-patterns-groupped.csv
  cat ${OUTPUT_DIR}/record-patterns.csv \
    | grep -v "\\$" \
    | sort \
    | uniq -c \
    | sort -n -r \
    | sed -r 's/^ *([0-9]+) /\1,/' >> ${OUTPUT_DIR}/record-patterns-groupped.csv
}

do_version_link() {
  printf "%s %s> [version-link]\n" $(date +"%F %T")
  if [[ "${VERSION:-}" != "" ]]; then
    OUTPUT_LINK=${BASE_OUTPUT_DIR}/${NAME}
    if [[ -e ${OUTPUT_LINK} ]]; then
      rm ${OUTPUT_LINK}
    fi
    ln -s ${OUTPUT_DIR} ${OUTPUT_LINK}
  fi
}

do_sqlite() {
  printf "%s %s> [SQLite]\n" $(date +"%F %T")

  printf "%s %s> php scripts/sqlite/normalize-issue-details.php ${OUTPUT_DIR} &> ${PREFIX}/sqlite.log\n" $(date +"%F %T")
  php scripts/sqlite/normalize-issue-details.php ${OUTPUT_DIR} &> ${PREFIX}/sqlite.log

  printf "%s %s> delete DB\n" $(date +"%F %T")
  if [[ -e ${OUTPUT_DIR}/qa_catalogue.sqlite ]]; then
    rm ${OUTPUT_DIR}/qa_catalogue.sqlite
  fi

  HAS_GROUP_PARAM=$(echo ${TYPE_PARAMS} | grep -c -P -e '--groupBy [^-]' || true)
  if [[ "${HAS_GROUP_PARAM}" == "0" ]]; then
    printf "%s %s> create DB structure\n" $(date +"%F %T")
    sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite < scripts/sqlite/qa_catalogue.sqlite.sql
  else
    printf "%s %s> create DB structure (groupped)\n" $(date +"%F %T")
    sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite < scripts/sqlite/qa_catalogue.groupped.sqlite.sql
  fi

  printf "%s %s> create importable files\n" $(date +"%F %T")
  tail -n +2 ${OUTPUT_DIR}/issue-details-normalized.csv > ${OUTPUT_DIR}/issue-details-normalized_noheader.csv
  tail -n +2 ${OUTPUT_DIR}/issue-summary.csv > ${OUTPUT_DIR}/issue-summary_noheader.csv
  if [[ "${HAS_GROUP_PARAM}" == "1" ]]; then
    tail -n +2 ${OUTPUT_DIR}/id-groupid.csv > ${OUTPUT_DIR}/id-groupid_noheader.csv
  fi

  printf "%s %s> import issue details\n" $(date +"%F %T")
  sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite << EOF
.mode csv
.import ${OUTPUT_DIR}/issue-details-normalized_noheader.csv issue_details
EOF

  printf "%s %s> import issue summary\n" $(date +"%F %T")
  sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite << EOF
.mode csv
.import ${OUTPUT_DIR}/issue-summary_noheader.csv issue_summary
EOF

  if [[ "${HAS_GROUP_PARAM}" == "1" ]]; then
    printf "%s %s> import id_groupid\n" $(date +"%F %T")
    sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite << EOF
.mode csv
.import ${OUTPUT_DIR}/id-groupid_noheader.csv id_groupid
EOF
  fi


  printf "%s %s> delete importable files\n" $(date +"%F %T")
  rm ${OUTPUT_DIR}/issue-details-normalized_noheader.csv
  rm ${OUTPUT_DIR}/issue-summary_noheader.csv
  if [[ "${HAS_GROUP_PARAM}" == "1" ]]; then
    rm ${OUTPUT_DIR}/id-groupid_noheader.csv
  fi

  if [[ "${HAS_GROUP_PARAM}" == "0" ]]; then
    printf "%s %s> index\n" $(date +"%F %T")
    sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite < scripts/sqlite/modify-tables.sql &>> ${PREFIX}/sqlite.log
  else
    printf "%s %s> index (groupped)\n" $(date +"%F %T")
    sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite < scripts/sqlite/modify-tables.groupped.sqlite.sql &>> ${PREFIX}/sqlite.log
    Rscript scripts/classifications/classifications-type.R ${OUTPUT_DIR} &>> ${PREFIX}/sqlite.log
  fi
}

do_mysql() {
  printf "%s %s> [MySQL]\n" $(date +"%F %T")

  printf "%s %s> php scripts/sqlite/normalize-issue-details.php ${OUTPUT_DIR} &> ${PREFIX}/mysql.log\n" $(date +"%F %T")
  php scripts/sqlite/normalize-issue-details.php ${OUTPUT_DIR} &> ${PREFIX}/mysql.log

  # printf "%s %s> delete DB\n" $(date +"%F %T")
  # if [[ -e ${OUTPUT_DIR}/qa_catalogue.sqlite ]]; then
  #   rm ${OUTPUT_DIR}/qa_catalogue.sqlite
  # fi
  mysql --defaults-extra-file=mysql.client.cnf ${NAME} -e "DROP TABLE IF EXISTS issue_details"
  mysql --defaults-extra-file=mysql.client.cnf ${NAME} -e "DROP TABLE IF EXISTS issue_summary"
  mysql --defaults-extra-file=mysql.client.cnf ${NAME} -e "DROP TABLE IF EXISTS issue_groups"
  mysql --defaults-extra-file=mysql.client.cnf ${NAME} -e "DROP TABLE IF EXISTS id_groupid"

  HAS_GROUP_PARAM=$(echo ${TYPE_PARAMS} | grep -c -P -e '--groupBy [^-]' || true)
  if [[ "${HAS_GROUP_PARAM}" == "0" ]]; then
    printf "%s %s> create DB structure\n" $(date +"%F %T")
    mysql --defaults-extra-file=mysql.client.cnf ${NAME} < scripts/sqlite/qa_catalogue.mysql.sql
  else
    printf "%s %s> create DB structure (groupped)\n" $(date +"%F %T")
    mysql --defaults-extra-file=mysql.client.cnf ${NAME} < scripts/sqlite/qa_catalogue.groupped.mysql.sql
  fi

  printf "%s %s> create importable files\n" $(date +"%F %T")
  # tail -n +2 ${OUTPUT_DIR}/issue-details-normalized.csv > ${OUTPUT_DIR}/issue-details-normalized_noheader.csv
  # tail -n +2 ${OUTPUT_DIR}/issue-summary.csv > ${OUTPUT_DIR}/issue-summary_noheader.csv
  # if [[ "${HAS_GROUP_PARAM}" == "1" ]]; then
  #   tail -n +2 ${OUTPUT_DIR}/id-groupid.csv > ${OUTPUT_DIR}/id-groupid_noheader.csv
  # fi

  printf "%s %s> import issue details\n" $(date +"%F %T")
  MYSQL_SEC_DIR=$(mysql --defaults-extra-file=mysql.client.cnf ${NAME} -e 'SHOW VARIABLES LIKE "secure_file_priv"\G' | grep 'Value:' | sed 's,\s*Value: ,,')
  sudo cp ${OUTPUT_DIR}/issue-details-normalized.csv ${MYSQL_SEC_DIR}
  mysql --defaults-extra-file=mysql.client.cnf ${NAME} -f << EOF
LOAD DATA INFILE '${MYSQL_SEC_DIR}/issue-details-normalized.csv' 
INTO TABLE issue_details
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"'
LINES TERMINATED BY '\n'
IGNORE 1 LINES;
EOF
  # sudo rm ${MYSQL_SEC_DIR}/issue-details-normalized.csv

  printf "%s %s> import issue summary\n" $(date +"%F %T")
  sudo cp ${OUTPUT_DIR}/issue-summary.csv ${MYSQL_SEC_DIR}
  mysql --defaults-extra-file=mysql.client.cnf ${NAME} -f << EOF
LOAD DATA INFILE '${MYSQL_SEC_DIR}/issue-summary.csv' 
INTO TABLE issue_summary
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"'
LINES TERMINATED BY '\n'
IGNORE 1 LINES;
EOF
  # sudo rm ${MYSQL_SEC_DIR}/issue-summary.csv

  if [[ "${HAS_GROUP_PARAM}" == "1" ]]; then
    printf "%s %s> import id_groupid\n" $(date +"%F %T")
    sudo cp ${OUTPUT_DIR}/id-groupid.csv ${MYSQL_SEC_DIR}
    sudo mysql --defaults-extra-file=mysql.client.cnf ${NAME} -f << EOF
LOAD DATA INFILE '${MYSQL_SEC_DIR}/id-groupid.csv' 
INTO TABLE id_groupid
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"'
LINES TERMINATED BY '\n'
IGNORE 1 LINES;
EOF
    # sudo rm ${MYSQL_SEC_DIR}/id-groupid.csv
  fi


  printf "%s %s> delete importable files\n" $(date +"%F %T")
  # rm ${OUTPUT_DIR}/issue-details-normalized_noheader.csv
  # rm ${OUTPUT_DIR}/issue-summary_noheader.csv
  # if [[ "${HAS_GROUP_PARAM}" == "1" ]]; then
  #   rm ${OUTPUT_DIR}/id-groupid_noheader.csv
  # fi

  if [[ "${HAS_GROUP_PARAM}" == "0" ]]; then
    printf "%s %s> index\n" $(date +"%F %T")
    sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite < scripts/sqlite/modify-tables.sql &>> ${PREFIX}/mysql.log
  else
    printf "%s %s> index (groupped)\n" $(date +"%F %T")
    mysql --defaults-extra-file=mysql.client.cnf ${NAME} < scripts/sqlite/modify-tables.groupped.mysql.sql &>> ${PREFIX}/mysql.log
  fi
}

do_export_schema_files() {
  if [[ ! -d marc-schema ]]; then
    mkdir marc-schema
  fi
  printf "%s %s> [avram]\n" $(date +"%F %T")
  ./export-schema --withSubfieldCodelists > marc-schema/marc-schema.json
  ./export-schema --withSubfieldCodelists --solrFieldType mixed --withSelfDescriptiveCode > marc-schema/marc-schema-with-solr.json
  ./export-schema --withSubfieldCodelists --solrFieldType mixed --withSelfDescriptiveCode --withLocallyDefinedFields > marc-schema/marc-schema-with-solr-and-extensions.json
  printf "%s %s> 3 files generated at 'marc-schema' directory: marc-schema.json, marc-schema-with-solr.json, marc-schema-with-solr-and-extensions.json\n" $(date +"%F %T")
}

do_shacl4bib() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  printf "%s %s> [shacl4bib]\n" $(date +"%F %T")
  printf "%s %s> ./shacl4bib --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/shacl4bib.log\n" $(date +"%F %T")
  ./shacl4bib --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/shacl4bib.log
}

do_all_analyses() {
  do_validate
  do_sqlite
  do_completeness
  do_classifications
  do_authorities
  do_tt_completeness
  do_shelf_ready_completeness
  do_serial_score
  do_functional_analysis
  # do_bl_classification
  # do_network_analysis
  do_pareto
  do_marc_history
  # do_sqlite
}

do_all_solr() {
  do_prepare_solr
  do_index
}

help() {
ME=$0
cat <<END
QA catalogue for analysing library data

usage: $ME [command]

commands:
  validate                   record validation
  completeness               completeness analysis
  classifications            classification analysis
  authorities                authority analysis
  tt-completeness            Thompson-Traill completeness analysis
  shelf-ready-completeness   shelf-ready completeness analysis
  bl-classification          British Library's quality classification
  serial-score               serial score analysis
  format                     search and format records
  functional-analysis        FRBR functional requirement analysis
  network-analysis           network analysis
  pareto                     pareto analysis
  marc-history               generating cataloguing history chart
  record-patterns            record patterns
  prepare-solr               prepare indexing
  index                      indexing with Solr
  sqlite                     import tables to SQLite
  export-schema-files        export schema files
  shacl4bib                  run SHACL-like validation
  all-analyses               run all analitical tasks
  all-solr                   run all indexing tasks
  all                        run all tasks
  help                       print this help message

END
}

case "$1" in
  validate)                 do_validate ; do_sqlite ;;
  prepare-solr)             do_prepare_solr ;;
  index)                    do_index ;;
  completeness)             do_completeness ;;
  classifications)          do_classifications ;;
  authorities)              do_authorities ;;
  tt-completeness)          do_tt_completeness ;;
  shelf-ready-completeness) do_shelf_ready_completeness ;;
  bl-classification)        do_bl_classification ;;
  serial-score)             do_serial_score ;;
  format)                   do_format ;;
  functional-analysis)      do_functional_analysis ;;
  network-analysis)         do_network_analysis ;;
  pareto)                   do_pareto ;;
  marc-history)             do_marc_history ;;
  record-patterns)          do_record_patterns ;;
  sqlite)                   do_sqlite ;;
  mysql)                    do_mysql ;;
  export-schema-files)      do_export_schema_files ;;
  all-analyses)             do_all_analyses ;;
  all-solr)                 do_all_solr ;;
  all)                      do_all_analyses ; do_all_solr ;;
  version-link)             do_version_link ;;
  help)                     help ;;
esac

if [[ "$1" != "help" ]]; then
  duration=$SECONDS
  hours=$(($duration / (60*60)))
  mins=$(($duration % (60*60) / 60))
  secs=$(($duration % 60))

  printf "%s %s> DONE. %02d:%02d:%02d elapsed.\n" $(date +"%F %T") $hours $mins $secs
fi
