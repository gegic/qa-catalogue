#!/usr/bin/env bash
set -uo pipefail

# when run interactively via terminal
if [ -t 1 ]; then
    # prefix each command with timestamp in trace mode
    PS4='\033[0D\033[1;37m$(date +"%F %T")>\033[0m '
    function colored() { echo -en "\033[${1}m${2}\033[0m"; }
else
    PS4='$(date +"%F %T")> '
    function colored() { echo -n "$2"; }
fi

# emit message with timestamp
log() {
  colored "1;37" "$(date +'%F %T')> "
  echo "$1"
}

# define 'untrace' command to disable trace mode
unalias -a
shopt -s expand_aliases
alias untrace='{ set +x; } 2> /dev/null'

# start a named processing step and enable trace mode
run() {
  untrace
  colored "1;1" "[$1]"
  echo ""
  set -x
}

# ---- proccessing steps ----

do_validate() {
  GENERAL_PARAMS="--details --trimId --summary --format csv --defaultRecordType BOOKS"
  OUTPUT_PARAMS="--outputDir ${OUTPUT_DIR} --detailsFileName issue-details.csv --summaryFileName issue-summary.csv"
  run validate
  ./validate ${GENERAL_PARAMS} ${OUTPUT_PARAMS} ${TYPE_PARAMS} ${MARC_DIR}/$MASK 2> ${PREFIX}/validate.log
}

do_prepare_solr() {
  run prepare-solr
  ./prepare-solr $NAME 2> ${PREFIX}/solr.log
}

do_index() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--ignorableIssueTypes [^ ]+//g')
  run index
  ./index --db $NAME --file-path ${MARC_DIR} --file-mask $MASK ${PARAMS} --trimId 2>> ${PREFIX}/solr.log
}

do_completeness() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  run completeness
  ./completeness --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/completeness.log
}

do_completeness_sqlite() {
  run completeness_sqlite

  untrace
  HAS_GROUP_PARAM=$(echo ${TYPE_PARAMS} | grep -c -P -e '--groupBy [^-]' || true)

  if [[ "${HAS_GROUP_PARAM}" == "1" ]]; then
    bash scripts/sqlite/completeness-groupped.sqlite.sh ${OUTPUT_DIR}
  fi
}

do_classifications() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  run classifications
  ./classifications --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/classifications.log
  Rscript scripts/classifications/classifications-type.R ${OUTPUT_DIR}
}

do_authorities() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  run authorities
  ./authorities --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/authorities.log
}

do_tt_completeness() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  run tt-completeness
  ./tt-completeness --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ --trimId ${MARC_DIR}/${MASK} 2> ${PREFIX}/tt-completeness.log
  Rscript scripts/tt-histogram/tt-histogram.R ${OUTPUT_DIR} &>> ${PREFIX}/tt-completeness.log

  # for large files
  # php scripts/tt-histogram/tt-histogram.php ${OUTPUT_DIR} &>> ${PREFIX}/tt-completeness.log
}

do_shelf_ready_completeness() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  run shelf-ready-completeness
  ./shelf-ready-completeness \
    --defaultRecordType BOOKS \
    ${PARAMS} \
    --outputDir ${OUTPUT_DIR}/ \
    --trimId ${MARC_DIR}/${MASK} 2> ${PREFIX}/shelf-ready-completeness.log

  Rscript scripts/shelf-ready/shelf-ready-histogram.R ${OUTPUT_DIR} &>> ${PREFIX}/shelf-ready-completeness.log

  # for large files
  # php scripts/shelf-ready-histogram.php ${OUTPUT_DIR} &>> ${PREFIX}/shelf-ready-completeness.log
}

do_bl_classification() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  run bk-classification
  ./bl-classification \
    --defaultRecordType BOOKS \
    ${PARAMS} \
    --outputDir ${OUTPUT_DIR}/ \
    --trimId ${MARC_DIR}/${MASK} 2> ${PREFIX}/bl-classification.log
}

do_serial_score() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  run serial-score
  ./serial-score --defaultRecordType BOOKS \
                 ${PARAMS} \
                 --outputDir ${OUTPUT_DIR}/ \
                 --trimId ${MARC_DIR}/${MASK} 2> ${PREFIX}/serial-score.log

  Rscript scripts/serial-score/serial-score-histogram.R ${OUTPUT_DIR} &>> ${PREFIX}/serial-score.log
}

do_format() {
  run format
  ./formatter --defaultRecordType BOOKS ${MARC_DIR}/${MASK}
}

do_functional_analysis() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  run functional-analysis
  ./functional-analysis --defaultRecordType BOOKS \
                        ${PARAMS} \
                        --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/functional-analysis.log
}

do_network_analysis() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  run network-analysis
  ./network-analysis --defaultRecordType BOOKS \
                     ${PARAMS} \
                     --outputDir ${OUTPUT_DIR}/ \
                     ${MARC_DIR}/${MASK} 2> ${PREFIX}/network-analysis.log

  # network.csv (concept, id) ->
  #   network-by-concepts.csv (concept, count, ids)
  #   network-by-record.csv (id, count, concepts)
  #   network-statistics.csv (type, total, single, multi)
  Rscript scripts/network-transform.R ${OUTPUT_DIR} &>> ${PREFIX}/network-analysis.log

  # network-by-concepts (concept, count, ids) ->
  #   network-pairs.csv (id1 id2)
  #   network-nodes.csv (id, id)
  ./network-analysis --outputDir ${OUTPUT_DIR} \
                     --action pairing \
                     &>> ${PREFIX}/network-analysis.log

  untrace

  cat network-pairs.csv | sort | uniq -c | sort -nr > network-pairs-uniq-with-count.csv
  awk '{print $2 " " $3}' network-pairs-uniq-with-count.csv > network-pairs-all.csv

  log "ziping output"
  PWD=`pdw`
  cd ${OUTPUT_DIR}
  zip network-input network-nodes.csv network-nodes-???.csv network-pairs-???.csv network-by-concepts-tags.csv
  cd $PWD

  log "upload output"
  scp ${OUTPUT_DIR}/network-input.zip pkiraly@roedel.etrap.eu:/roedel/pkiraly/network/input

  # spark-shell -I scripts/network.scala --conf spark.driver.metadata.qa.dir="${OUTPUT_DIR}"
  # ./network-export.sh ${OUTPUT_DIR}
}

do_pareto() {
  run pareto
  Rscript scripts/pareto/frequency-range.R ${OUTPUT_DIR} &> ${PREFIX}/pareto.log
  untrace

  . ./common-variables
  if [[ "${WEB_DIR:-}" != "" ]]; then
    mkdir -p $WEB_DIR/images
    ln -s ${OUTPUT_DIR}/img $WEB_DIR/images/${NAME}
  fi
}

do_marc_history() {
  if [ "${SCHEMA:-}" == "PICA" ]; then
    SELECTOR='011@$a;001A$0|extractPicaDate'
  else
    SELECTOR="008~7-10;008~0-5"
  fi
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')

  run marc-history
  ./formatter --selector "$SELECTOR" --defaultRecordType BOOKS ${PARAMS} --separator "," \
              --outputDir ${OUTPUT_DIR}/ --fileName "marc-history.csv" ${MARC_DIR}/${MASK} &> ${PREFIX}/marc-history.log
    # | grep -v '008~7-10,008~0-5' \
  
  untrace
  tail -n +2 ${OUTPUT_DIR}/marc-history.csv \
    | sort \
    | uniq -c \
    | sed -r 's/([0-9]) ([0-9uc xwticrka])/\1,\2/' \
    | sed 's, ,,g' > ${OUTPUT_DIR}/marc-history-groupped.csv 
  set -x

  Rscript scripts/marc-history/marc-history-groupped.R ${OUTPUT_DIR} &>> ${PREFIX}/marc-history.log
}

do_record_patterns() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')

  run record-patterns
  Rscript scripts/record-patterns/top-fields.R ${OUTPUT_DIR} &>> ${PREFIX}/top-fields.log
  ./record-patterns --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} &> ${PREFIX}/record-patterns.log

  head -1 ${OUTPUT_DIR}/record-patterns.csv | sed -e 's/^/count,/' > ${OUTPUT_DIR}/record-patterns-groupped.csv
  cat ${OUTPUT_DIR}/record-patterns.csv \
    | grep -v "\\$" \
    | sort \
    | uniq -c \
    | sort -n -r \
    | sed -r 's/^ *([0-9]+) /\1,/' >> ${OUTPUT_DIR}/record-patterns-groupped.csv
}

do_version_link() {
  run version-link

  untrace
  if [[ "${VERSION:-}" != "" ]]; then
    OUTPUT_LINK=${BASE_OUTPUT_DIR}/${NAME}
    if [[ -e ${OUTPUT_LINK} ]]; then
      rm ${OUTPUT_LINK}
    fi
    ln -s ${OUTPUT_DIR} ${OUTPUT_LINK}
  fi
}

do_sqlite() {
  run sqlite
  php scripts/sqlite/normalize-issue-details.php ${OUTPUT_DIR} &> ${PREFIX}/sqlite.log

  untrace
  log "delete DB"

  if [[ -e ${OUTPUT_DIR}/qa_catalogue.sqlite ]]; then
    rm ${OUTPUT_DIR}/qa_catalogue.sqlite
  fi

  HAS_GROUP_PARAM=$(echo ${TYPE_PARAMS} | grep -c -P -e '--groupBy [^-]' || true)
  if [[ "${HAS_GROUP_PARAM}" == "0" ]]; then
    log "create DB structure"
    sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite < scripts/sqlite/qa_catalogue.sqlite.sql
  else
    log "create DB structure (groupped)"
    sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite < scripts/sqlite/qa_catalogue.groupped.sqlite.sql
  fi

  log "create importable files"
  tail -n +2 ${OUTPUT_DIR}/issue-details-normalized.csv > ${OUTPUT_DIR}/issue-details-normalized_noheader.csv
  tail -n +2 ${OUTPUT_DIR}/issue-summary.csv > ${OUTPUT_DIR}/issue-summary_noheader.csv
  if [[ "${HAS_GROUP_PARAM}" == "1" ]]; then
    tail -n +2 ${OUTPUT_DIR}/id-groupid.csv > ${OUTPUT_DIR}/id-groupid_noheader.csv
  fi

  log "import issue details"
  sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite << EOF
.mode csv
.import ${OUTPUT_DIR}/issue-details-normalized_noheader.csv issue_details
EOF

  log "import issue summary"
  sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite << EOF
.mode csv
.import ${OUTPUT_DIR}/issue-summary_noheader.csv issue_summary
EOF

  if [[ "${HAS_GROUP_PARAM}" == "1" ]]; then
    log "import id_groupid"
    sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite << EOF
.mode csv
.import ${OUTPUT_DIR}/id-groupid_noheader.csv id_groupid
EOF
  fi

  log "delete importable files"
  rm ${OUTPUT_DIR}/issue-details-normalized_noheader.csv
  rm ${OUTPUT_DIR}/issue-summary_noheader.csv
  if [[ "${HAS_GROUP_PARAM}" == "1" ]]; then
    rm ${OUTPUT_DIR}/id-groupid_noheader.csv
  fi

  if [[ "${HAS_GROUP_PARAM}" == "0" ]]; then
    log "index"
    sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite < scripts/sqlite/modify-tables.sql &>> ${PREFIX}/sqlite.log
  else
    log "index (groupped)"
    scripts/sqlite/calculate-aggregated-numbers.groupped.sh ${OUTPUT_DIR}

#    sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite < scripts/sqlite/modify-tables.groupped.sqlite.sql &>> ${PREFIX}/sqlite.log
#    Rscript scripts/sqlite/qa_catalogue.groupping.R ${OUTPUT_DIR} &>> ${PREFIX}/sqlite.log

#    log "import issue_group_types"
#    tail -n +2 ${OUTPUT_DIR}/issue-groupped-types.csv > ${OUTPUT_DIR}/issue-groupped-types-noheader.csv
#  sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite << EOF
#.mode csv
#.import ${OUTPUT_DIR}/issue-groupped-types-noheader.csv issue_group_types
#EOF

#    log "import issue_group_categories"
#    tail -n +2 ${OUTPUT_DIR}/issue-groupped-categories.csv > ${OUTPUT_DIR}/issue-groupped-categories-noheader.csv
#  sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite << EOF
#.mode csv
#.import ${OUTPUT_DIR}/issue-groupped-categories-noheader.csv issue_group_categories
#EOF

#    log "import issue_group_paths"
#    tail -n +2 ${OUTPUT_DIR}/issue-groupped-paths.csv > ${OUTPUT_DIR}/issue-groupped-paths-noheader.csv
#  sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite << EOF
#.mode csv
#.import ${OUTPUT_DIR}/issue-groupped-paths-noheader.csv issue_group_paths
#EOF

#    rm ${OUTPUT_DIR}/issue-groupped-types-noheader.csv
#    rm ${OUTPUT_DIR}/issue-groupped-categories-noheader.csv
#    rm ${OUTPUT_DIR}/issue-groupped-paths-noheader.csv
  fi
}

do_mysql() {
  run mysql

  php scripts/sqlite/normalize-issue-details.php ${OUTPUT_DIR} &> ${PREFIX}/mysql.log

  untrace

  # log "delete DB"
  # if [[ -e ${OUTPUT_DIR}/qa_catalogue.sqlite ]]; then
  #   rm ${OUTPUT_DIR}/qa_catalogue.sqlite
  # fi
  mysql --defaults-extra-file=mysql.client.cnf ${NAME} -e "DROP TABLE IF EXISTS issue_details"
  mysql --defaults-extra-file=mysql.client.cnf ${NAME} -e "DROP TABLE IF EXISTS issue_summary"
  mysql --defaults-extra-file=mysql.client.cnf ${NAME} -e "DROP TABLE IF EXISTS issue_groups"
  mysql --defaults-extra-file=mysql.client.cnf ${NAME} -e "DROP TABLE IF EXISTS id_groupid"

  HAS_GROUP_PARAM=$(echo ${TYPE_PARAMS} | grep -c -P -e '--groupBy [^-]' || true)
  if [[ "${HAS_GROUP_PARAM}" == "0" ]]; then
    log "create DB structure"
    mysql --defaults-extra-file=mysql.client.cnf ${NAME} < scripts/sqlite/qa_catalogue.mysql.sql
  else
    log "create DB structure (groupped)"
    mysql --defaults-extra-file=mysql.client.cnf ${NAME} < scripts/sqlite/qa_catalogue.groupped.mysql.sql
  fi

  log "create importable files"
  # tail -n +2 ${OUTPUT_DIR}/issue-details-normalized.csv > ${OUTPUT_DIR}/issue-details-normalized_noheader.csv
  # tail -n +2 ${OUTPUT_DIR}/issue-summary.csv > ${OUTPUT_DIR}/issue-summary_noheader.csv
  # if [[ "${HAS_GROUP_PARAM}" == "1" ]]; then
  #   tail -n +2 ${OUTPUT_DIR}/id-groupid.csv > ${OUTPUT_DIR}/id-groupid_noheader.csv
  # fi

  log "import issue details"
  MYSQL_SEC_DIR=$(mysql --defaults-extra-file=mysql.client.cnf ${NAME} -e 'SHOW VARIABLES LIKE "secure_file_priv"\G' | grep 'Value:' | sed 's,\s*Value: ,,')
  sudo cp ${OUTPUT_DIR}/issue-details-normalized.csv ${MYSQL_SEC_DIR}
  mysql --defaults-extra-file=mysql.client.cnf ${NAME} -f << EOF
LOAD DATA INFILE '${MYSQL_SEC_DIR}/issue-details-normalized.csv' 
INTO TABLE issue_details
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"'
LINES TERMINATED BY '\n'
IGNORE 1 LINES;
EOF
  # sudo rm ${MYSQL_SEC_DIR}/issue-details-normalized.csv

  log "import issue summary"
  sudo cp ${OUTPUT_DIR}/issue-summary.csv ${MYSQL_SEC_DIR}
  mysql --defaults-extra-file=mysql.client.cnf ${NAME} -f << EOF
LOAD DATA INFILE '${MYSQL_SEC_DIR}/issue-summary.csv' 
INTO TABLE issue_summary
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"'
LINES TERMINATED BY '\n'
IGNORE 1 LINES;
EOF
  # sudo rm ${MYSQL_SEC_DIR}/issue-summary.csv

  if [[ "${HAS_GROUP_PARAM}" == "1" ]]; then
    log "import id_groupid"
    sudo cp ${OUTPUT_DIR}/id-groupid.csv ${MYSQL_SEC_DIR}
    sudo mysql --defaults-extra-file=mysql.client.cnf ${NAME} -f << EOF
LOAD DATA INFILE '${MYSQL_SEC_DIR}/id-groupid.csv' 
INTO TABLE id_groupid
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"'
LINES TERMINATED BY '\n'
IGNORE 1 LINES;
EOF
    # sudo rm ${MYSQL_SEC_DIR}/id-groupid.csv
  fi


  log "delete importable files"
  # rm ${OUTPUT_DIR}/issue-details-normalized_noheader.csv
  # rm ${OUTPUT_DIR}/issue-summary_noheader.csv
  # if [[ "${HAS_GROUP_PARAM}" == "1" ]]; then
  #   rm ${OUTPUT_DIR}/id-groupid_noheader.csv
  # fi

  if [[ "${HAS_GROUP_PARAM}" == "0" ]]; then
    log "index"
    sqlite3 ${OUTPUT_DIR}/qa_catalogue.sqlite < scripts/sqlite/modify-tables.sql &>> ${PREFIX}/mysql.log
  else
    log "index (groupped)"
    mysql --defaults-extra-file=mysql.client.cnf ${NAME} < scripts/sqlite/modify-tables.groupped.mysql.sql &>> ${PREFIX}/mysql.log
  fi
}

do_export_schema_files() {
  mkdir -p marc-schema
  run avram

  untrace
  ./export-schema --withSubfieldCodelists > marc-schema/marc-schema.json
  ./export-schema --withSubfieldCodelists --solrFieldType mixed --withSelfDescriptiveCode > marc-schema/marc-schema-with-solr.json
  ./export-schema --withSubfieldCodelists --solrFieldType mixed --withSelfDescriptiveCode --withLocallyDefinedFields > marc-schema/marc-schema-with-solr-and-extensions.json

  log "files generated at 'marc-schema' directory: marc-schema.json, marc-schema-with-solr.json, marc-schema-with-solr-and-extensions.json"
}

do_shacl4bib() {
  PARAMS=$(echo ${TYPE_PARAMS} | sed -r 's/\s*--emptyLargeCollectors|\s*--with-delete|\s*--ignorableIssueTypes [^ ]+//g')
  run shacl4bib
  ./shacl4bib --defaultRecordType BOOKS ${PARAMS} --outputDir ${OUTPUT_DIR}/ ${MARC_DIR}/${MASK} 2> ${PREFIX}/shacl4bib.log
}

do_all_analyses() {
  tasks=$(echo "${ANALYSES}" | tr , ' ')
  for task in $tasks; do
      declare -F "do_$task" > /dev/null || fatal "unknown analysis task: $task"
  done
  for task in $(echo "${ANALYSES}" | tr , ' '); do
    do_$task
  done
}

do_all_solr() {
  do_prepare_solr
  do_index
}

# ---- usage and execution of proccessing steps ----

help() {
cat <<END
QA catalogue for analysing library data

usage: $0 [command]

commands:
  validate                   record validation
  completeness               completeness analysis
  classifications            classification analysis
  authorities                authority analysis
  tt-completeness            Thompson-Traill completeness analysis
  shelf-ready-completeness   shelf-ready completeness analysis
  bl-classification          British Library's quality classification
  serial-score               serial score analysis
  format                     search and format records
  functional-analysis        FRBR functional requirement analysis
  network-analysis           network analysis
  pareto                     pareto analysis
  marc-history               generating cataloguing history chart
  record-patterns            record patterns
  prepare-solr               prepare indexing
  index                      indexing with Solr
  sqlite                     import tables to SQLite
  completeness-sqlite        import groupped output of completeness to SQLite
  export-schema-files        export schema files
  shacl4bib                  run SHACL-like validation
  all-analyses               run all analytical tasks (or those set via ANALYSES)
  all-solr                   run all indexing tasks
  all                        run all tasks
  config                     show configuration
  help                       print this help message

END
}

config() {
  set +u
  echo "NAME=$NAME"
  echo "MARC_DIR=$MARC_DIR"
  echo "MASK=$MASK"
  echo "OUTPUT_DIR=$OUTPUT_DIR"
  echo "TYPE_PARAMS=$TYPE_PARAMS"
  echo "ANALYSES=$ANALYSES"
  echo "WEB_DIR=$WEB_DIR"
  echo "PREFIX=$PREFIX"
  cat ./common-variables
}

fatal() {
  colored "1;31" "$1"
  exit 1
}

# initialize environment
NAME=${NAME:-$(basename $0 .sh)}
BASE_INPUT_DIR=${BASE_INPUT_DIR:-./input}
BASE_OUTPUT_DIR=${BASE_OUTPUT_DIR:-./output}
MARC_DIR=${MARC_DIR:-$BASE_INPUT_DIR/$NAME}

PREFIX=${BASE_INPUT_DIR}/_reports/$NAME
if [[ "${VERSION:-}" != "" ]]; then
  OUTPUT_DIR=${BASE_OUTPUT_DIR}/${NAME}-${VERSION}
else
  OUTPUT_DIR=${BASE_OUTPUT_DIR}/${NAME}
fi

# which tasks to run on `all-analyses`
ALL_ANALYSES=validate,sqlite,completeness,completeness_sqlite,classifications,authorities,tt_completeness,shelf_ready_completeness,serial_score,functional_analysis,pareto,marc_history
ANALYSES=${ANALYSES:-$ALL_ANALYSES}

# check directories for processing commands
if [[ ! "${1:-help}" =~ ^(help|config|export-schema-files)$ ]]; then
  cmd=$1

  mkdir -p $PREFIX
  mkdir -p $OUTPUT_DIR

  log "input:  $MARC_DIR/$MASK"
  log "output: $OUTPUT_DIR"
  log "logs:   $PREFIX"

  ls ${MARC_DIR}/${MASK} &> /dev/null || fatal "Missing input files!"

  if [[ ! -z "${UPDATE:-}" ]]; then
    log "update: $UPDATE"
    echo "${UPDATE}" > "${OUTPUT_DIR}/last-update.csv"
  fi
fi

case "${1:-help}" in
  validate)                 do_validate ; do_sqlite ;;
  prepare-solr)             do_prepare_solr ;;
  index)                    do_index ;;
  completeness)             do_completeness ; do_completeness_sqlite ;;
  classifications)          do_classifications ;;
  authorities)              do_authorities ;;
  tt-completeness)          do_tt_completeness ;;
  shelf-ready-completeness) do_shelf_ready_completeness ;;
  bl-classification)        do_bl_classification ;;
  serial-score)             do_serial_score ;;
  format)                   do_format ;;
  functional-analysis)      do_functional_analysis ;;
  network-analysis)         do_network_analysis ;;
  pareto)                   do_pareto ;;
  marc-history)             do_marc_history ;;
  record-patterns)          do_record_patterns ;;
  completeness-sqlite)      do_completeness_sqlite ;;
  sqlite)                   do_sqlite ;;
  mysql)                    do_mysql ;;
  export-schema-files)      do_export_schema_files ;;
  all-analyses)             do_all_analyses ;;
  all-solr)                 do_all_solr ;;
  all)                      do_all_analyses ; do_all_solr ;;
  version-link)             do_version_link ;;
  config)                   config ;;
  help)                     help ;;
  *)                        fatal "unknown command: $1"
esac

untrace

if [ ! -z "${cmd:-}" ]; then
  sec=$SECONDS
  log "DONE in $(printf '%02d:%02d:%02d\n' $((sec/3600)) $((sec%3600/60)) $((sec%60)))"
fi
